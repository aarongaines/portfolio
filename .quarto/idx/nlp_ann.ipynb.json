{"title":"Natural Language Processing (NLP) and Neural Networks with NLTK and the Tensorflow Keras API","markdown":{"headingText":"Natural Language Processing (NLP) and Neural Networks with NLTK and the Tensorflow Keras API","containsRefs":false,"markdown":"\n\n### Introduction\n\nIn this notebook, I will be building a model to predict whether or not a tweet is about a real disaster, using the data given for the assignment. The data has been provided in separate training and test sets, so only the training set will be used in the model training/validation while the test set will be saved for the end to evalute the model.\n\nFirst the data will be loaded and explored, then the data will be cleaned and preprocessed, and finally the model will be built and trained. I decided to add some features to the data set (e.g. number of words in a tweet) to see if they would improve the model's performance. I also used the prebuild sentiment analysis model from the NLTK library to see if it would improve the model's performance. Finally I used the sklearn library to vectorize the text data with the TF-IDF vectorizer and the nltk tweet tokenizer to tokenize the text data. After all of this, I used the tensorflow.keras library to build and train the model.\n\n### Import Libraries\n\n### Custom Functions\n\nlist of custom functions:\n\n1. *dtype_convert*: converts the data type of a column in a dataframe\n2. *link_count*: if a link exists in a tweet\n2. *get_polarity*: returns the polarity of a tweet using the NLTK sentiment analysis model\n2. *return_xy*: returns the x and y data from a dataframe\n2. *return_vectorized_data*: returns the vectorized text and extra features from a dataframe\n2. *plot_history*: plots the training and validation accuracy and loss from a history object\n2. *plot_roc*: plots the ROC curve using the best saved model and the test data\n\n### Import Data\n\n### Add Features\n\n### Tokenize and Vectorize Data\n\n### Train Model\n\nFirst we convert the dataframes to numpy arrays to work with the keras api. I used the functional api to build the model. It has two hidden layers, a concatenated layer of the original input and previous hidden layers, and one more hidden layer before the output layer. I used the Adam optimizer and the binary crossentropy loss function. I also used the early stopping and checkpoint callbacks to save the best model. I used the fit method to train the model.\n\nAbove is a plot of the training and validation accuracy and loss. The model seems to be overfitting more as the epochs increase, and we can see the best epoch for loss is around 6, where the best epoch for accuracy is around 7. This indicates that the model may be overfitting and we should stop training the model at around 6 epochs or 7 epochs. However, the model may also be improved by changing the optimizer, learning rate, adding more layers or the amount of training data avaliable.\n\n### Evaluate Model\n\nAbove we see the results for our test data set. The model has an AUC of .862, which is not bad. However, from the ROC AUC plot above we can see that the model is still overfitting to the training data and is not generalizing to the test data as well as it could. This could be due to many factors, such as the amount or accuracy of training data, the amount of features, the model architecture, the learning rate, and the amount of epochs. However, the model is still able to predict wheather or not a tweet is about a real disaster with a decent accuracy.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"nlp_ann.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"spacelab","max-width":"2800px"},"extensions":{"book":{"multiFile":true}}}}}