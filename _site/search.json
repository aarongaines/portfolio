[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Aaron Gaines, and I am 29 years old. I have been working as a research assistant at California State University, Northridge (CSUN) since October 2019, where I also received a Bachelors in Geography - Geographic Information Science in June 2021. I have also recently received a graduate certificate in Data Science from the UCLA Extension program.\n\nExperience\nMost of my time as a research assistant has been spent working on updating the National Hydrography Dataset (NHD), a USGS database, for the California Department of Water Resources. This involved creating, editing, and managing digitized hydrographic features while maintaining a minimum level of quality required by the USGS to ensure data interoperability between different state agency’s update efforts. Overtime my responsibilities within the team grew from simple first pass editing to managing a job from start to finish, including performing quality control checks on other editor’s work to ensure our updates can be merged into the main database.\nMore recently I have been working on a pilot project investigating the possible effects of sea level rise on groundwater levels and contamination, as well as disadvantaged communities (DACs). This has involved doing literature reviews on similar projects, aggregating, and presenting that information to a larger team. With feedback I then would apply the methodologies found in different papers to our data to compare and discover the different advantages and drawbacks. An example of this was to look at how different state agencies would designate DACs. From this the group decided on a methodology that seemed to be the most complete, which happened to be the CalEnviroScreen (CES). I then used their white paper to emulate some of their methodologies in determining environmental and social disadvantages with python and ArcGIS Pro. For example, spatial analysis of monitoring wells and their sample data to determine a blockgroup’s exposure to specific groundwater contamination. When we found their determination of data quality not to our liking, I used the American Housing Survey variance replicate data to determine the uncertainty in our social vulnerability index, a method described by (et al.). This project has grown my skills in literature review, effectively presenting technical papers and methodologies and emulation of said methodologies with Python.\n\n\nSkills\nI am very proficient in the use of the ESRI stack of products, particularly ArcGIS Pro and ArcGIS Online as well as automating tasks using the ArcGIS model builder and ArcPy. While I do not have a lot of experience with QGIS, that is something I would like to change as I have a fondness for opensource technology for the simple fact that it allows others to replicate and expand on the work of others more easily. I also have a decent amount of experience in Python and many of its data science libraries including Pandas, Matplotlib, Scikit-Learn and the Tensorflow Keras API. I have no problem writing custom classes or modules/packages where necessary. I also have some experience with PostgreSQL and PostGIS queries.\n\n\nWhy GIS and Data Science?\nI enjoy looking at problems from a big picture perspective, and I really enjoy how GIS and data science let me explore and solve problems concerning such a wide range of industries and topics. And while I am constantly learning new things directly related to my work, like tricking machine learning models, I also enjoy learning about a vast range of other topics related to science. Whether its material science or astrophysics, I am always intrigued by the process solving puzzles and discovering new things."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Hi there, my name is Aaron Gaines. This website serves as somewhat of an online portfolio for me. Hopefully you will get a better idea of who I am and the skills I can bring to your projects. The portfolio page contains links to different projects I have completed with the goal of demonstrating as many of my skills in GIS and data science as possible. Please check out my About Me page for more information on who I am and check my Contact links for ways to get in touch with me."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Below is a list of links to different projects of mine that I have selected to hopefully provide a demonstration of the breadth of skills and knowledge I have gained throughout my acedemic and professional experience."
  },
  {
    "objectID": "portfolio.html#projects",
    "href": "portfolio.html#projects",
    "title": "Portfolio",
    "section": "Projects",
    "text": "Projects\nNatural Language Processing and Neural Network Classification\nAnother Placeholder Project"
  },
  {
    "objectID": "projects/nlp_ann.html",
    "href": "projects/nlp_ann.html",
    "title": "Aaron Gaines",
    "section": "",
    "text": "Natural Language Processing (NLP) and Neural Networks with NLTK and the Tensorflow Keras API\n\nIntroduction\nIn this notebook, I will be building a model to predict whether or not a tweet is about a real disaster, using the data given for the assignment. The data has been provided in separate training and test sets, so only the training set will be used in the model training/validation while the test set will be saved for the end to evalute the model.\nFirst the data will be loaded and explored, then the data will be cleaned and preprocessed, and finally the model will be built and trained. I decided to add some features to the data set (e.g. number of words in a tweet) to see if they would improve the model’s performance. I also used the prebuild sentiment analysis model from the NLTK library to see if it would improve the model’s performance. Finally I used the sklearn library to vectorize the text data with the TF-IDF vectorizer and the nltk tweet tokenizer to tokenize the text data. After all of this, I used the tensorflow.keras library to build and train the model.\n\n\nImport Libraries\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import TweetTokenizer\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_curve\nimport random\n\n\nrandom.seed(42)\n\n\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nTrue\n\n\n\nstop_words = set(stopwords.words('english'))\n\n\n\nCustom Functions\n\ndtype_convert: converts the data type of a column in a dataframe\nlink_count: if a link exists in a tweet\nget_polarity: returns the polarity of a tweet using the NLTK sentiment analysis model\nreturn_xy: returns the x and y data from a dataframe\nreturn_vectorized_data: returns the vectorized text and extra features from a dataframe\nplot_history: plots the training and validation accuracy and loss from a history object\nplot_roc: plots the ROC curve using the best saved model and the test data\n\n\ndef dtype_convert(data : pd.DataFrame):\n    \"\"\"Converts the data types of the numeric columns in a dataframe to the smallest possible type.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame to have its numeric columns converted.\n    \"\"\"\n\n    int_list = ['int8', 'int16', 'int32', 'int64']\n    float_list = ['float16', 'float32', 'float64']\n\n    for col in data.select_dtypes(include=np.number).columns:\n\n        data_type = data[col].dtype\n        \n        # Convert to int\n        if data_type in int_list:\n\n            n_range = [abs(data[col].max()), abs(data[col].min())]\n            n_max = max(n_range)\n\n            if n_max <= 127:\n                data[col] = data[col].astype(np.int8)\n\n            elif n_max <= 32767:\n                data[col] = data[col].astype(np.int16)\n\n            elif n_max <= 2147483647:\n                data[col] = data[col].astype(np.int32)\n\n            else:\n                data[col] = data[col].astype(np.int64)\n\n        # Convert to float\n        elif data_type in float_list:\n                \n                n_range = [abs(data[col].max()), abs(data[col].min())]\n                n_max = max(n_range)\n    \n                if n_max <= 3.4e+38:\n                    data[col] = data[col].astype(np.float32)\n    \n                else:\n                    data[col] = data[col].astype(np.float64)\n    \n    return data\n        \n\n\ndef link_count(text : str):\n    \"\"\"Returns binary value indicating whether a tweet contains a link or not.\n    \n    Parameters\n    ----------\n    text : str to be checked for links.\"\"\"\n\n    # Checks string for http\n    links = text.count('http')\n\n    if links > 0:\n        return 1\n    \n    else:\n        return 0\n\n\ndef get_polarity(row : pd.Series):\n    \"\"\"Returns the polarity of a tweet using the VADER sentiment analyzer from NLTK.\n    \n    Parameters\n    ----------\n    row : pd.Series (to be applied on rows of pd.DataFrame) containing the text of a tweet.\"\"\"\n\n    sia = SentimentIntensityAnalyzer()\n\n    text = row['text']\n    polarity = sia.polarity_scores(text)\n\n    neg = polarity['neg']\n    neu = polarity['neu']\n    pos = polarity['pos']\n    compound = polarity['compound']\n\n    return neg, neu, pos, compound\n\n\ndef return_xy(data):\n\n    # Add new features including polarity scores\n    # Binary feature for whether a tweet contains a link\n    data['links_f'] = data['text'].apply(link_count).astype(np.int8)\n    \n    data['tweet_len'] = data['text'].str.len()\n    data['word_count'] = data['text'].str.split().str.len()\n    data['word_density'] = data['tweet_len'] / (data['word_count'])\n\n    # Get polarity scores\n    print('Getting polarity scores...')\n    data['neg'], data['neu'], data['pos'], data['compound_f'] = zip(*data.apply(get_polarity, axis=1))\n\n    # Convert dtypes to save memory\n    #data['target'] = data['target'].astype('bool')\n    data = dtype_convert(data)\n    \n    # Columns for X and y\n    X = data.drop('target', axis=1)\n    y = data['target'].astype('int8')\n\n    return X, y\n\n\ndef return_vectorized_data(train : pd.DataFrame, test : pd.DataFrame, text : list, features : list,\n                           max_features=None, stop_words=None):\n    \"\"\"Returns vectorized data for training and testing.\n\n    Parameters\n    ----------\n    train : pd.DataFrame containing training data.\n    test : pd.DataFrame containing testing data.\n    text : str indicating the column name of the text data.\n    features : list of str indicating the column names of the numeric features.\n    max_features : int indicating the maximum number of features to be used in the vectorizer.\n    stop_words : list of str indicating the stop words to be used in the vectorizer.\n    \"\"\"\n\n    tk = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n\n    vectorizer = TfidfVectorizer(tokenizer=tk.tokenize, strip_accents='unicode', max_features=max_features, stop_words=stop_words)\n\n    v_train = vectorizer.fit_transform(train[text])\n    v_test = vectorizer.transform(test[text])\n\n    v_train = pd.DataFrame(v_train.toarray(), columns=vectorizer.get_feature_names_out(), index=train.index)\n    v_test = pd.DataFrame(v_test.toarray(), columns=vectorizer.get_feature_names_out(), index=test.index)\n\n    v_train = v_train[v_train.columns].astype(np.float32)\n    v_test = v_test[v_test.columns].astype(np.float32)\n\n    v_train = v_train.join(train[features], how='left', lsuffix='_v', rsuffix='_f')\n    v_test = v_test.join(test[features], how='left', lsuffix='_v', rsuffix='_f')\n\n    return v_train, v_test\n\n\ndef plot_history(history):\n    \"\"\"Plots the training and validation loss and accuracy for a model.\n\n    Parameters\n    ----------\n    history : Keras history object.\n    \"\"\"\n\n    fig, ax = plt.subplots(2, 1, figsize=(11, 11))\n\n    auc_best = np.max(history.history['val_auc'])\n    auc_best_epoch = np.argmax(history.history['val_auc'])\n    auc_max = history.history['auc'][auc_best_epoch]\n\n\n    ax[0].plot(history.history['auc'], label='Train')\n    ax[0].plot(history.history['val_auc'], label='Validation')\n    ax[0].vlines(auc_best_epoch, 0, np.max(history.history['val_auc']), color='purple', linestyles='dashed', label='Best Epoch')\n\n    ax[0].annotate(f'{auc_best - np.max(history.history[\"auc\"]):.4f}',\n                    xy=(auc_best_epoch, auc_max),\n                    xytext=(auc_best_epoch, auc_best-0.05),\n                    arrowprops=dict(facecolor='black', shrink=0.05, width=0.7, headwidth=5))\n\n    ax[0].set_ylabel('AUC')\n    #ax[0].set_title('AUC vs. Epoch')\n    ax[0].legend(loc='lower left')\n\n    loss_best = np.min(history.history['val_loss'])\n    loss_best_epoch = np.argmin(history.history['val_loss'])\n    loss_max = history.history['loss'][loss_best_epoch]\n\n    ax[1].plot(history.history['loss'], label='Train')\n    ax[1].plot(history.history['val_loss'], label='Validation')\n    ax[1].vlines(loss_best_epoch, 0, loss_best, color='purple', linestyles='dashed', label='Best Epoch')\n\n    ax[1].annotate(f'{loss_best - np.min(history.history[\"loss\"]):.4f}',\n                    xy=(loss_best_epoch, loss_max),\n                    xytext=(loss_best_epoch, loss_best+.01),\n                    arrowprops=dict(facecolor='black', shrink=0, width=0.7, headwidth=5))\n\n    ax[1].set_xlabel('Epoch')\n    ax[1].set_ylabel('Loss')\n    #ax[1].set_title('Loss vs. Epoch')\n\n    ax[1].legend(loc='lower left')\n\n    plt.show()\n\n\ndef plot_roc(X_train, X_test, y_train, y_test, model):\n    \"\"\"Plots the ROC AUC curve for a model.\n\n    Parameters\n    ----------\n    X_train : pd.DataFrame or numpy array containing the training data.\n    X_test : pd.DataFrame or numpy array containing the testing data.\n    y_train : pd.Series or numpy array containing the training target.\n    y_test : pd.Series or numpy array containing the testing target.\n    model : Keras model.\n    \"\"\"\n    \n    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, model.predict(X_train))\n    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, model.predict(X_test))\n\n    # Set figure size\n    plt.figure(figsize=(10, 10))\n\n    plt.plot(fpr_train, tpr_train, label='Train')\n    plt.plot(fpr_test, tpr_test, label='Test')\n\n    plt.legend(['Train', 'Test'], loc='upper left')\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC AUC Curve')\n    plt.show()\n\n\n\nImport Data\n\ntrain_data = pd.read_csv('data/bonus_train_data-2.csv')\ntest_data = pd.read_csv('data/bonus_test_data-2.csv')\n\n\n\nAdd Features\n\nX_train, y_train = return_xy(train_data)\nX_test, y_test = return_xy(test_data)\n\nfeatures = ['tweet_len', 'word_count', 'word_density', 'links_f', 'neg', 'neu', 'pos', 'compound_f']\n\nGetting polarity scores...\nGetting polarity scores...\n\n\n\n\nTokenize and Vectorize Data\n\n# Runs the custom function to vectorize the data\nX_train_v, X_test_v = return_vectorized_data(X_train, X_test, 'text', features=features, max_features=None, stop_words=stop_words)\n\n\nX_train_v[features]\n\n\n\n\n\n  \n    \n      \n      tweet_len\n      word_count\n      word_density\n      links_f\n      neg\n      neu\n      pos\n      compound_f\n    \n  \n  \n    \n      0\n      140\n      18\n      7.777778\n      1\n      0.207\n      0.536\n      0.257\n      0.3182\n    \n    \n      1\n      98\n      15\n      6.533333\n      0\n      0.205\n      0.795\n      0.000\n      -0.4767\n    \n    \n      2\n      143\n      22\n      6.500000\n      1\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      3\n      118\n      17\n      6.941176\n      1\n      0.138\n      0.862\n      0.000\n      -0.3736\n    \n    \n      4\n      82\n      9\n      9.111111\n      0\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6085\n      96\n      16\n      6.000000\n      0\n      0.000\n      0.502\n      0.498\n      0.9169\n    \n    \n      6086\n      132\n      28\n      4.714286\n      0\n      0.276\n      0.724\n      0.000\n      -0.8126\n    \n    \n      6087\n      121\n      13\n      9.307693\n      1\n      0.155\n      0.845\n      0.000\n      -0.2960\n    \n    \n      6088\n      136\n      20\n      6.800000\n      1\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      6089\n      101\n      17\n      5.941176\n      1\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n  \n\n6090 rows × 8 columns\n\n\n\n\n\nScale Data\n\n# import sklearn standard scaler\nfrom sklearn.preprocessing import StandardScaler\n\n\nX_train_f = X_train_v[features]\nX_test_f = X_test_v[features]\n\n# instantiate the scaler\nscaler = StandardScaler()\n\n# fit transform the training data\nX_train_fs = scaler.fit_transform(X_train_f)\nX_test_fs = scaler.transform(X_test_f)\n\nX_train_fs = pd.DataFrame(X_train_fs, columns=features, index=X_train_f.index)\nX_test_fs = pd.DataFrame(X_test_fs, columns=features, index=X_test_f.index)\n\nX_train_v[features] = X_train_fs[features]\nX_test_v[features] = X_test_fs[features]\n\n\n\nTrain Model\nFirst we convert the dataframes to numpy arrays to work with the keras api. I used the functional api to build the model. It has two hidden layers, a concatenated layer of the original input and previous hidden layers, and one more hidden layer before the output layer. I used the Adam optimizer and the binary crossentropy loss function. I also used the early stopping and checkpoint callbacks to save the best model. I used the fit method to train the model.\n\ntrain_text = X_train_v.drop(features, axis=1)\ntest_text = X_test_v.drop(features, axis=1)\n\ntrain_feats = X_train_v[features]\ntest_feats = X_test_v[features]\n\n\ntrain_text_n = train_text.to_numpy()\ntest_text_n = test_text.to_numpy()\n\ntrain_feats_n = train_feats.to_numpy()\ntest_feats_n = test_feats.to_numpy()\n\ny_train_n = y_train.to_numpy()\ny_test_n = y_test.to_numpy()\n\n\ntf.keras.backend.clear_session()\n\nact1 = tf.keras.layers.LeakyReLU(alpha=0.05)\n\ntext_input_ = tf.keras.layers.Input(shape=(train_text_n.shape[1],), name='text_input')\ntext_layers = tf.keras.layers.Dense(32, activation=act1)(text_input_)\n#text_layers = tf.keras.layers.Dropout(0.2)(text_layers)\n\nfeats_input_ = tf.keras.layers.Input(shape=(train_feats_n.shape[1],), name='feats_input')\nfeats_layers = tf.keras.layers.Dense(32, activation=act1)(feats_input_)\nfeats_layers = tf.keras.layers.Dropout(0.1)(feats_layers)\n\nconcat_l = tf.keras.layers.Concatenate()([text_layers, feats_layers])\nconcat_i = tf.keras.layers.Concatenate()([text_input_, feats_input_])\n\nconcat = tf.keras.layers.Concatenate()([concat_l, concat_i])\nconcat = tf.keras.layers.Dropout(0.1)(concat)\n\noutput_ = tf.keras.layers.Dense(1, activation='sigmoid')(concat)\n\nmodel = tf.keras.Model(inputs=[text_input_, feats_input_], outputs=output_)\n\nmodel.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n feats_input (InputLayer)       [(None, 8)]          0           []                               \n                                                                                                  \n text_input (InputLayer)        [(None, 17781)]      0           []                               \n                                                                                                  \n dense_1 (Dense)                (None, 32)           288         ['feats_input[0][0]']            \n                                                                                                  \n dense (Dense)                  (None, 32)           569024      ['text_input[0][0]']             \n                                                                                                  \n dropout (Dropout)              (None, 32)           0           ['dense_1[0][0]']                \n                                                                                                  \n concatenate (Concatenate)      (None, 64)           0           ['dense[0][0]',                  \n                                                                  'dropout[0][0]']                \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 17789)        0           ['text_input[0][0]',             \n                                                                  'feats_input[0][0]']            \n                                                                                                  \n concatenate_2 (Concatenate)    (None, 17853)        0           ['concatenate[0][0]',            \n                                                                  'concatenate_1[0][0]']          \n                                                                                                  \n dropout_1 (Dropout)            (None, 17853)        0           ['concatenate_2[0][0]']          \n                                                                                                  \n dense_2 (Dense)                (None, 1)            17854       ['dropout_1[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 587,166\nTrainable params: 587,166\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\nopt = tf.keras.optimizers.Adamax(learning_rate=0.0003)\nmetric = tf.keras.metrics.AUC()\n\nmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=metric)\n\n\nimport absl.logging\nabsl.logging.set_verbosity(absl.logging.ERROR)\n\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=4, mode='max'),\n             tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min'),\n             tf.keras.callbacks.ModelCheckpoint(filepath='checkpoint.hd5', monitor='val_auc', save_best_only=True, mode='max')\n             ]\n\nmodel_results = model.fit([train_text, train_feats], y_train_n, validation_split=0.02,\n                          epochs=100, batch_size=64, callbacks=callbacks,\n                          use_multiprocessing=True)\n\nEpoch 1/100\n94/94 [==============================] - ETA: 0s - loss: 0.6810 - auc: 0.6380WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n\n\nWARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.true_positives\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.true_positives\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.true_negatives\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.true_negatives\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.false_positives\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.false_positives\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.false_negatives\n\n\nWARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.false_negatives\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 6s 62ms/step - loss: 0.6810 - auc: 0.6380 - val_loss: 0.6689 - val_auc: 0.7194\nEpoch 2/100\n94/94 [==============================] - ETA: 0s - loss: 0.6648 - auc: 0.6938INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 53ms/step - loss: 0.6648 - auc: 0.6938 - val_loss: 0.6518 - val_auc: 0.7375\nEpoch 3/100\n94/94 [==============================] - ETA: 0s - loss: 0.6507 - auc: 0.7144INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 6s 62ms/step - loss: 0.6507 - auc: 0.7144 - val_loss: 0.6373 - val_auc: 0.7471\nEpoch 4/100\n94/94 [==============================] - ETA: 0s - loss: 0.6384 - auc: 0.7262INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.6384 - auc: 0.7262 - val_loss: 0.6247 - val_auc: 0.7533\nEpoch 5/100\n94/94 [==============================] - ETA: 0s - loss: 0.6270 - auc: 0.7351INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 53ms/step - loss: 0.6270 - auc: 0.7351 - val_loss: 0.6133 - val_auc: 0.7591\nEpoch 6/100\n94/94 [==============================] - ETA: 0s - loss: 0.6184 - auc: 0.7403INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 53ms/step - loss: 0.6184 - auc: 0.7403 - val_loss: 0.6036 - val_auc: 0.7644\nEpoch 7/100\n93/94 [============================>.] - ETA: 0s - loss: 0.6095 - auc: 0.7484INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.6096 - auc: 0.7482 - val_loss: 0.5964 - val_auc: 0.7674\nEpoch 8/100\n94/94 [==============================] - ETA: 0s - loss: 0.6030 - auc: 0.7537INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 57ms/step - loss: 0.6030 - auc: 0.7537 - val_loss: 0.5890 - val_auc: 0.7726\nEpoch 9/100\n94/94 [==============================] - ETA: 0s - loss: 0.5949 - auc: 0.7618INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 50ms/step - loss: 0.5949 - auc: 0.7618 - val_loss: 0.5819 - val_auc: 0.7769\nEpoch 10/100\n93/94 [============================>.] - ETA: 0s - loss: 0.5877 - auc: 0.7691INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 51ms/step - loss: 0.5878 - auc: 0.7689 - val_loss: 0.5755 - val_auc: 0.7815\nEpoch 11/100\n93/94 [============================>.] - ETA: 0s - loss: 0.5813 - auc: 0.7759INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 56ms/step - loss: 0.5809 - auc: 0.7763 - val_loss: 0.5692 - val_auc: 0.7864\nEpoch 12/100\n94/94 [==============================] - ETA: 0s - loss: 0.5736 - auc: 0.7841INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.5736 - auc: 0.7841 - val_loss: 0.5631 - val_auc: 0.7933\nEpoch 13/100\n94/94 [==============================] - ETA: 0s - loss: 0.5668 - auc: 0.7900INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 56ms/step - loss: 0.5668 - auc: 0.7900 - val_loss: 0.5573 - val_auc: 0.7985\nEpoch 14/100\n94/94 [==============================] - ETA: 0s - loss: 0.5586 - auc: 0.8002INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 51ms/step - loss: 0.5586 - auc: 0.8002 - val_loss: 0.5519 - val_auc: 0.8027\nEpoch 15/100\n94/94 [==============================] - ETA: 0s - loss: 0.5521 - auc: 0.8063INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.5521 - auc: 0.8063 - val_loss: 0.5463 - val_auc: 0.8057\nEpoch 16/100\n94/94 [==============================] - ETA: 0s - loss: 0.5435 - auc: 0.8155INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 51ms/step - loss: 0.5435 - auc: 0.8155 - val_loss: 0.5410 - val_auc: 0.8100\nEpoch 17/100\n94/94 [==============================] - ETA: 0s - loss: 0.5365 - auc: 0.8215INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 50ms/step - loss: 0.5365 - auc: 0.8215 - val_loss: 0.5358 - val_auc: 0.8153\nEpoch 18/100\n93/94 [============================>.] - ETA: 0s - loss: 0.5297 - auc: 0.8292INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 50ms/step - loss: 0.5296 - auc: 0.8293 - val_loss: 0.5306 - val_auc: 0.8203\nEpoch 19/100\n94/94 [==============================] - ETA: 0s - loss: 0.5220 - auc: 0.8372INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.5220 - auc: 0.8372 - val_loss: 0.5259 - val_auc: 0.8244\nEpoch 20/100\n94/94 [==============================] - ETA: 0s - loss: 0.5129 - auc: 0.8460INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.5129 - auc: 0.8460 - val_loss: 0.5210 - val_auc: 0.8288\nEpoch 21/100\n93/94 [============================>.] - ETA: 0s - loss: 0.5063 - auc: 0.8524INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.5062 - auc: 0.8525 - val_loss: 0.5168 - val_auc: 0.8313\nEpoch 22/100\n94/94 [==============================] - ETA: 0s - loss: 0.4981 - auc: 0.8606INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 55ms/step - loss: 0.4981 - auc: 0.8606 - val_loss: 0.5122 - val_auc: 0.8350\nEpoch 23/100\n94/94 [==============================] - ETA: 0s - loss: 0.4925 - auc: 0.8650INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 56ms/step - loss: 0.4925 - auc: 0.8650 - val_loss: 0.5079 - val_auc: 0.8392\nEpoch 24/100\n93/94 [============================>.] - ETA: 0s - loss: 0.4836 - auc: 0.8727INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.4836 - auc: 0.8727 - val_loss: 0.5034 - val_auc: 0.8432\nEpoch 25/100\n94/94 [==============================] - ETA: 0s - loss: 0.4751 - auc: 0.8802INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.4751 - auc: 0.8802 - val_loss: 0.4988 - val_auc: 0.8456\nEpoch 26/100\n94/94 [==============================] - ETA: 0s - loss: 0.4688 - auc: 0.8844INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.4688 - auc: 0.8844 - val_loss: 0.4940 - val_auc: 0.8486\nEpoch 27/100\n94/94 [==============================] - ETA: 0s - loss: 0.4604 - auc: 0.8905INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 53ms/step - loss: 0.4604 - auc: 0.8905 - val_loss: 0.4900 - val_auc: 0.8512\nEpoch 28/100\n94/94 [==============================] - ETA: 0s - loss: 0.4525 - auc: 0.8967INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.4525 - auc: 0.8967 - val_loss: 0.4857 - val_auc: 0.8558\nEpoch 29/100\n94/94 [==============================] - ETA: 0s - loss: 0.4445 - auc: 0.9021INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.4445 - auc: 0.9021 - val_loss: 0.4817 - val_auc: 0.8577\nEpoch 30/100\n94/94 [==============================] - ETA: 0s - loss: 0.4377 - auc: 0.9059INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 56ms/step - loss: 0.4377 - auc: 0.9059 - val_loss: 0.4774 - val_auc: 0.8584\nEpoch 31/100\n94/94 [==============================] - ETA: 0s - loss: 0.4297 - auc: 0.9114INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.4297 - auc: 0.9114 - val_loss: 0.4734 - val_auc: 0.8603\nEpoch 32/100\n94/94 [==============================] - ETA: 0s - loss: 0.4219 - auc: 0.9162INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 55ms/step - loss: 0.4219 - auc: 0.9162 - val_loss: 0.4692 - val_auc: 0.8610\nEpoch 33/100\n94/94 [==============================] - ETA: 0s - loss: 0.4148 - auc: 0.9199INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 55ms/step - loss: 0.4148 - auc: 0.9199 - val_loss: 0.4651 - val_auc: 0.8622\nEpoch 34/100\n94/94 [==============================] - ETA: 0s - loss: 0.4059 - auc: 0.9245INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.4059 - auc: 0.9245 - val_loss: 0.4612 - val_auc: 0.8638\nEpoch 35/100\n94/94 [==============================] - ETA: 0s - loss: 0.3997 - auc: 0.9284INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 53ms/step - loss: 0.3997 - auc: 0.9284 - val_loss: 0.4576 - val_auc: 0.8652\nEpoch 36/100\n94/94 [==============================] - ETA: 0s - loss: 0.3924 - auc: 0.9314INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.3924 - auc: 0.9314 - val_loss: 0.4539 - val_auc: 0.8675\nEpoch 37/100\n93/94 [============================>.] - ETA: 0s - loss: 0.3850 - auc: 0.9346INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 53ms/step - loss: 0.3852 - auc: 0.9346 - val_loss: 0.4506 - val_auc: 0.8693\nEpoch 38/100\n93/94 [============================>.] - ETA: 0s - loss: 0.3794 - auc: 0.9370INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 57ms/step - loss: 0.3791 - auc: 0.9372 - val_loss: 0.4475 - val_auc: 0.8704\nEpoch 39/100\n94/94 [==============================] - ETA: 0s - loss: 0.3722 - auc: 0.9406INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.3722 - auc: 0.9406 - val_loss: 0.4440 - val_auc: 0.8726\nEpoch 40/100\n94/94 [==============================] - ETA: 0s - loss: 0.3652 - auc: 0.9434INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.3652 - auc: 0.9434 - val_loss: 0.4410 - val_auc: 0.8738\nEpoch 41/100\n94/94 [==============================] - ETA: 0s - loss: 0.3596 - auc: 0.9452INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.3596 - auc: 0.9452 - val_loss: 0.4380 - val_auc: 0.8769\nEpoch 42/100\n93/94 [============================>.] - ETA: 0s - loss: 0.3525 - auc: 0.9480INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.3522 - auc: 0.9481 - val_loss: 0.4351 - val_auc: 0.8785\nEpoch 43/100\n93/94 [============================>.] - ETA: 0s - loss: 0.3456 - auc: 0.9509INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 53ms/step - loss: 0.3457 - auc: 0.9509 - val_loss: 0.4325 - val_auc: 0.8789\nEpoch 44/100\n94/94 [==============================] - ETA: 0s - loss: 0.3399 - auc: 0.9521INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 57ms/step - loss: 0.3399 - auc: 0.9521 - val_loss: 0.4301 - val_auc: 0.8798\nEpoch 45/100\n94/94 [==============================] - ETA: 0s - loss: 0.3344 - auc: 0.9542INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 50ms/step - loss: 0.3344 - auc: 0.9542 - val_loss: 0.4274 - val_auc: 0.8820\nEpoch 46/100\n94/94 [==============================] - 4s 41ms/step - loss: 0.3291 - auc: 0.9553 - val_loss: 0.4250 - val_auc: 0.8816\nEpoch 47/100\n93/94 [============================>.] - ETA: 0s - loss: 0.3229 - auc: 0.9580INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.3227 - auc: 0.9581 - val_loss: 0.4229 - val_auc: 0.8827\nEpoch 48/100\n94/94 [==============================] - ETA: 0s - loss: 0.3187 - auc: 0.9588INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 51ms/step - loss: 0.3187 - auc: 0.9588 - val_loss: 0.4208 - val_auc: 0.8835\nEpoch 49/100\n94/94 [==============================] - ETA: 0s - loss: 0.3125 - auc: 0.9610INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 54ms/step - loss: 0.3125 - auc: 0.9610 - val_loss: 0.4188 - val_auc: 0.8845\nEpoch 50/100\n94/94 [==============================] - 4s 44ms/step - loss: 0.3074 - auc: 0.9619 - val_loss: 0.4167 - val_auc: 0.8834\nEpoch 51/100\n94/94 [==============================] - 4s 40ms/step - loss: 0.3024 - auc: 0.9634 - val_loss: 0.4150 - val_auc: 0.8843\nEpoch 52/100\n94/94 [==============================] - ETA: 0s - loss: 0.2978 - auc: 0.9645INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 48ms/step - loss: 0.2978 - auc: 0.9645 - val_loss: 0.4132 - val_auc: 0.8846\nEpoch 53/100\n94/94 [==============================] - ETA: 0s - loss: 0.2929 - auc: 0.9663INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.2929 - auc: 0.9663 - val_loss: 0.4114 - val_auc: 0.8852\nEpoch 54/100\n94/94 [==============================] - ETA: 0s - loss: 0.2885 - auc: 0.9669INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 5s 52ms/step - loss: 0.2885 - auc: 0.9669 - val_loss: 0.4099 - val_auc: 0.8856\nEpoch 55/100\n93/94 [============================>.] - ETA: 0s - loss: 0.2846 - auc: 0.9680INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 4s 48ms/step - loss: 0.2843 - auc: 0.9681 - val_loss: 0.4082 - val_auc: 0.8860\nEpoch 56/100\n94/94 [==============================] - 4s 43ms/step - loss: 0.2788 - auc: 0.9698 - val_loss: 0.4067 - val_auc: 0.8850\nEpoch 57/100\n94/94 [==============================] - 4s 43ms/step - loss: 0.2757 - auc: 0.9701 - val_loss: 0.4053 - val_auc: 0.8854\nEpoch 58/100\n94/94 [==============================] - 4s 42ms/step - loss: 0.2695 - auc: 0.9721 - val_loss: 0.4038 - val_auc: 0.8857\nEpoch 59/100\n94/94 [==============================] - 4s 42ms/step - loss: 0.2668 - auc: 0.9727 - val_loss: 0.4029 - val_auc: 0.8856\n\n\n\nplot_history(model_results)\n\n\n\n\nAbove is a plot of the training and validation accuracy and loss. The model seems to be overfitting more as the epochs increase, and we can see the best epoch for loss is around 6, where the best epoch for accuracy is around 7. This indicates that the model may be overfitting and we should stop training the model at around 6 epochs or 7 epochs. However, the model may also be improved by changing the optimizer, learning rate, adding more layers or the amount of training data avaliable.\n\n\nEvaluate Model\n\nmodel.load_weights('checkpoint.hd5')\n\nmodel.evaluate([test_text, test_feats], y_test_n)\n\n48/48 [==============================] - 2s 47ms/step - loss: 0.4407 - auc: 0.8647\n\n\n[0.4406600296497345, 0.8646914958953857]\n\n\n\nplot_roc([train_text, train_feats], [test_text, test_feats], y_train_n, y_test_n, model)\n\n191/191 [==============================] - 9s 49ms/step\n48/48 [==============================] - 2s 49ms/step\n\n\n\n\n\nAbove we see the results for our test data set. The model has an AUC of .865, which is not bad. However, from the ROC AUC plot above we can see that the model is still overfitting to the training data and is not generalizing to the test data as well as it could. This could be due to many factors, such as the amount or accuracy of training data, the amount of features, the model architecture, the learning rate, and the amount of epochs. However, the model is still able to predict wheather or not a tweet is about a real disaster with a decent accuracy."
  }
]