[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Aaron Gaines, and I am 29 years old. I have been working as a research assistant at California State University, Northridge (CSUN) since October 2019, where I also received a Bachelors in Geography - Geographic Information Science in June 2021. I have also recently received a graduate certificate in Data Science from the UCLA Extension program.\n\nExperience\nMost of my time as a research assistant has been spent working on updating the National Hydrography Dataset (NHD), a USGS database, for the California Department of Water Resources. This involved creating, editing, and managing digitized hydrographic features while maintaining a minimum level of quality required by the USGS to ensure data interoperability between different state agency’s update efforts. Overtime my responsibilities within the team grew from simple first pass editing to managing a job from start to finish, including performing quality control checks on other editor’s work to ensure our updates can be merged into the main database.\nMore recently I have been working on a pilot project investigating the possible effects of sea level rise on groundwater levels and contamination, as well as disadvantaged communities (DACs). This has involved doing literature reviews on similar projects, aggregating, and presenting that information to a larger team. With feedback I then would apply the methodologies found in different papers to our data to compare and discover the different advantages and drawbacks. An example of this was to look at how different state agencies would designate DACs. From this the group decided on a methodology that seemed to be the most complete, which happened to be the CalEnviroScreen (CES). I then used their white paper to emulate some of their methodologies in determining environmental and social disadvantages with python and ArcGIS Pro. For example, spatial analysis of monitoring wells and their sample data to determine a blockgroup’s exposure to specific groundwater contamination. When we found their determination of data quality not to our liking, I used the American Housing Survey variance replicate data to determine the uncertainty in our social vulnerability index, a method described by (et al.). This project has grown my skills in literature review, effectively presenting technical papers and methodologies and emulation of said methodologies with Python.\n\n\nSkills\nI am very proficient in the use of the ESRI stack of products, particularly ArcGIS Pro and ArcGIS Online as well as automating tasks using the ArcGIS model builder and ArcPy. While I do not have a lot of experience with QGIS, that is something I would like to change as I have a fondness for opensource technology for the simple fact that it allows others to replicate and expand on the work of others more easily. I also have a decent amount of experience in Python and many of its data science libraries including Pandas, Matplotlib, Scikit-Learn and the Tensorflow Keras API. I have no problem writing custom classes or modules/packages where necessary. I also have some experience with PostgreSQL and PostGIS queries.\n\n\nWhy GIS and Data Science?\nI enjoy looking at problems from a big picture perspective, and I really enjoy how GIS and data science let me explore and solve problems concerning such a wide range of industries and topics. And while I am constantly learning new things directly related to my work, like tricking machine learning models, I also enjoy learning about a vast range of other topics related to science. Whether its material science or astrophysics, I am always intrigued by the process solving puzzles and discovering new things."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Hi there, my name is Aaron Gaines. This website serves as somewhat of an online portfolio for me. Hopefully you will get a better idea of who I am and the skills I can bring to your projects. The portfolio page contains links to different projects I have completed with the goal of demonstrating as many of my skills in GIS and data science as possible. Please check out my About Me page for more information on who I am and check my Contact links for ways to get in touch with me."
  },
  {
    "objectID": "nlp_ann.html",
    "href": "nlp_ann.html",
    "title": "Aaron Gaines",
    "section": "",
    "text": "Natural Language Processing (NLP) and Neural Networks with NLTK and the Tensorflow Keras API\n\nIntroduction\nIn this notebook, I will be building a model to predict whether or not a tweet is about a real disaster, using the data given for the assignment. The data has been provided in separate training and test sets, so only the training set will be used in the model training/validation while the test set will be saved for the end to evalute the model.\nFirst the data will be loaded and explored, then the data will be cleaned and preprocessed, and finally the model will be built and trained. I decided to add some features to the data set (e.g. number of words in a tweet) to see if they would improve the model’s performance. I also used the prebuild sentiment analysis model from the NLTK library to see if it would improve the model’s performance. Finally I used the sklearn library to vectorize the text data with the TF-IDF vectorizer and the nltk tweet tokenizer to tokenize the text data. After all of this, I used the tensorflow.keras library to build and train the neural network. The metrics used to evaluate the model were the accuracy score and receiver operating characteristic (ROC) curve.\n\n\nImport Libraries\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import TweetTokenizer\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_curve\nimport random\n\n\nrandom.seed(42)\n\n\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nTrue\n\n\n\nstop_words = set(stopwords.words('english'))\n\n\n\nCustom Functions\n\ndtype_convert: converts the data type of a column in a dataframe\nlink_count: if a link exists in a tweet\nget_polarity: returns the polarity of a tweet using the NLTK sentiment analysis model\nreturn_xy: returns the x and y data from a dataframe\nreturn_vectorized_data: returns the vectorized text and extra features from a dataframe\nplot_history: plots the training and validation accuracy and loss from a history object\nplot_roc: plots the ROC curve using the best saved model and the test data\n\n\ndef dtype_convert(data : pd.DataFrame):\n    \"\"\"Converts the data types of the numeric columns in a dataframe to the smallest possible type.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame to have its numeric columns converted.\n    \"\"\"\n\n    int_list = ['int8', 'int16', 'int32', 'int64']\n    float_list = ['float16', 'float32', 'float64']\n\n    for col in data.select_dtypes(include=np.number).columns:\n\n        data_type = data[col].dtype\n        \n        # Convert to int\n        if data_type in int_list:\n\n            n_range = [abs(data[col].max()), abs(data[col].min())]\n            n_max = max(n_range)\n\n            if n_max <= 127:\n                data[col] = data[col].astype(np.int8)\n\n            elif n_max <= 32767:\n                data[col] = data[col].astype(np.int16)\n\n            elif n_max <= 2147483647:\n                data[col] = data[col].astype(np.int32)\n\n            else:\n                data[col] = data[col].astype(np.int64)\n\n        # Convert to float\n        elif data_type in float_list:\n                \n                n_range = [abs(data[col].max()), abs(data[col].min())]\n                n_max = max(n_range)\n    \n                if n_max <= 3.4e+38:\n                    data[col] = data[col].astype(np.float32)\n    \n                else:\n                    data[col] = data[col].astype(np.float64)\n    \n    return data\n        \n\n\ndef link_count(text : str):\n    \"\"\"Returns binary value indicating whether a tweet contains a link or not.\n    \n    Parameters\n    ----------\n    text : str to be checked for links.\"\"\"\n\n    # Checks string for http\n    links = text.count('http')\n\n    if links > 0:\n        return 1\n    \n    else:\n        return 0\n\n\ndef get_polarity(row : pd.Series):\n    \"\"\"Returns the polarity of a tweet using the VADER sentiment analyzer from NLTK.\n    \n    Parameters\n    ----------\n    row : pd.Series (to be applied on rows of pd.DataFrame) containing the text of a tweet.\"\"\"\n\n    sia = SentimentIntensityAnalyzer()\n\n    text = row['text']\n    polarity = sia.polarity_scores(text)\n\n    neg = polarity['neg']\n    neu = polarity['neu']\n    pos = polarity['pos']\n    compound = polarity['compound']\n\n    return neg, neu, pos, compound\n\n\ndef return_xy(data):\n\n    # Add new features including polarity scores\n    # Binary feature for whether a tweet contains a link\n    data['links_f'] = data['text'].apply(link_count).astype(np.int8)\n    \n    data['tweet_len'] = data['text'].str.len()\n    data['word_count'] = data['text'].str.split().str.len()\n    data['word_density'] = data['tweet_len'] / (data['word_count'])\n\n    # Get polarity scores\n    print('Getting polarity scores...')\n    data['neg'], data['neu'], data['pos'], data['compound_f'] = zip(*data.apply(get_polarity, axis=1))\n\n    # Convert dtypes to save memory\n    #data['target'] = data['target'].astype('bool')\n    data = dtype_convert(data)\n    \n    # Columns for X and y\n    X = data.drop('target', axis=1)\n    y = data['target'].astype('int8')\n\n    return X, y\n\n\ndef return_vectorized_data(train : pd.DataFrame, test : pd.DataFrame, text : list, features : list,\n                           max_features=None, stop_words=None):\n    \"\"\"Returns vectorized data for training and testing.\n\n    Parameters\n    ----------\n    train : pd.DataFrame containing training data.\n    test : pd.DataFrame containing testing data.\n    text : str indicating the column name of the text data.\n    features : list of str indicating the column names of the numeric features.\n    max_features : int indicating the maximum number of features to be used in the vectorizer.\n    stop_words : list of str indicating the stop words to be used in the vectorizer.\n    \"\"\"\n\n    tk = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n\n    vectorizer = TfidfVectorizer(tokenizer=tk.tokenize, strip_accents='unicode', max_features=max_features, stop_words=stop_words)\n\n    v_train = vectorizer.fit_transform(train[text])\n    v_test = vectorizer.transform(test[text])\n\n    v_train = pd.DataFrame(v_train.toarray(), columns=vectorizer.get_feature_names_out(), index=train.index)\n    v_test = pd.DataFrame(v_test.toarray(), columns=vectorizer.get_feature_names_out(), index=test.index)\n\n    v_train = v_train[v_train.columns].astype(np.float32)\n    v_test = v_test[v_test.columns].astype(np.float32)\n\n    v_train = v_train.join(train[features], how='left', lsuffix='_v', rsuffix='_f')\n    v_test = v_test.join(test[features], how='left', lsuffix='_v', rsuffix='_f')\n\n    return v_train, v_test\n\n\ndef plot_history(history):\n    \"\"\"Plots the training and validation loss and accuracy for a model.\n\n    Parameters\n    ----------\n    history : Keras history object.\n    \"\"\"\n\n    fig, ax = plt.subplots(2, 1, figsize=(11, 11))\n\n    auc_best = np.max(history.history['val_auc'])\n    auc_best_epoch = np.argmax(history.history['val_auc'])\n    auc_max = history.history['auc'][auc_best_epoch]\n\n\n    ax[0].plot(history.history['auc'], label='Train')\n    ax[0].plot(history.history['val_auc'], label='Validation')\n    ax[0].vlines(auc_best_epoch, 0, np.max(history.history['val_auc']), color='purple', linestyles='dashed', label='Best Epoch')\n\n    ax[0].annotate(f'{auc_best - np.max(history.history[\"auc\"]):.4f}',\n                    xy=(auc_best_epoch, auc_max),\n                    xytext=(auc_best_epoch, auc_best-0.05),\n                    arrowprops=dict(facecolor='black', shrink=0.05, width=0.7, headwidth=5))\n\n    ax[0].set_ylabel('AUC')\n    #ax[0].set_title('AUC vs. Epoch')\n    ax[0].legend(loc='lower left')\n\n    loss_best = np.min(history.history['val_loss'])\n    loss_best_epoch = np.argmin(history.history['val_loss'])\n    loss_max = history.history['loss'][loss_best_epoch]\n\n    ax[1].plot(history.history['loss'], label='Train')\n    ax[1].plot(history.history['val_loss'], label='Validation')\n    ax[1].vlines(loss_best_epoch, 0, loss_best, color='purple', linestyles='dashed', label='Best Epoch')\n\n    ax[1].annotate(f'{loss_best - np.min(history.history[\"loss\"]):.4f}',\n                    xy=(loss_best_epoch, loss_max),\n                    xytext=(loss_best_epoch, loss_best+.01),\n                    arrowprops=dict(facecolor='black', shrink=0, width=0.7, headwidth=5))\n\n    ax[1].set_xlabel('Epoch')\n    ax[1].set_ylabel('Loss')\n    #ax[1].set_title('Loss vs. Epoch')\n\n    ax[1].legend(loc='lower left')\n\n    plt.show()\n\n\ndef plot_roc(X_train, X_test, y_train, y_test, model):\n    \"\"\"Plots the ROC AUC curve for a model.\n\n    Parameters\n    ----------\n    X_train : pd.DataFrame or numpy array containing the training data.\n    X_test : pd.DataFrame or numpy array containing the testing data.\n    y_train : pd.Series or numpy array containing the training target.\n    y_test : pd.Series or numpy array containing the testing target.\n    model : Keras model.\n    \"\"\"\n    \n    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, model.predict(X_train))\n    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, model.predict(X_test))\n\n    # Set figure size\n    plt.figure(figsize=(10, 10))\n\n    plt.plot(fpr_train, tpr_train, label='Train')\n    plt.plot(fpr_test, tpr_test, label='Test')\n\n    plt.legend(['Train', 'Test'], loc='upper left')\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC AUC Curve')\n    plt.show()\n\n\n\nImport Data\n\ntrain_data = pd.read_csv('projects/data/bonus_train_data-2.csv')\ntest_data = pd.read_csv('projects/data/bonus_test_data-2.csv')\n\nNow that the data has been loaded, I will explore it to get an idea of what I am working with. It is important to make sure that the data is clean and that there are no missing values. I will then print some random samples from the data to get an idea of what the tweets look like. We may see some instances where the target is 1 (real disaster) but the tweet is not about a real disaster. This is important to keep in mind when training the model that we might be limited by the quality of the data.\n\ntrain_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6090 entries, 0 to 6089\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    6090 non-null   object\n 1   target  6090 non-null   int64 \ndtypes: int64(1), object(1)\nmemory usage: 95.3+ KB\n\n\n\ntrain_data\n\n\n\n\n\n  \n    \n      \n      text\n      target\n    \n  \n  \n    \n      0\n      Courageous and honest analysis of need to use ...\n      1\n    \n    \n      1\n      @ZachZaidman @670TheScore wld b a shame if tha...\n      0\n    \n    \n      2\n      Tell @BarackObama to rescind medals of 'honor'...\n      1\n    \n    \n      3\n      Worried about how the CA drought might affect ...\n      1\n    \n    \n      4\n      @YoungHeroesID Lava Blast &amp; Power Red #Pan...\n      0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      6085\n      @Eganator2000 There aren't many Obliteration s...\n      0\n    \n    \n      6086\n      just had a panic attack bc I don't have enough...\n      0\n    \n    \n      6087\n      Omron HEM-712C Automatic Blood Pressure Monito...\n      0\n    \n    \n      6088\n      Officials say a quarantine is in place at an A...\n      1\n    \n    \n      6089\n      I moved to England five years ago today. What ...\n      1\n    \n  \n\n6090 rows × 2 columns\n\n\n\n\nfor i in range(1, 9):\n    rand = np.random.randint(0, len(train_data)-1)\n    text = train_data.loc[rand, 'text']\n    target = train_data.loc[rand, 'target']\n    print(f'Index: {rand}')\n    print(f'Target: {target}\\n{text}\\n')\n\nIndex: 3895\nTarget: 0\n@MistressPip I'm amazed you have not been inundated mistress.\n\nIndex: 5303\nTarget: 1\nNew York City Outbreak: What Is Legionnaire's Disease?   http://t.co/CXI82rFiFS\n\nIndex: 1459\nTarget: 0\nI feel like death\n\nIndex: 5454\nTarget: 1\n#ABCNews Obama Declares Disaster for Typhoon-Devastated Saipan: Obama signs disaster declaration for No... http://t.co/DOBZc3piTM #World\n\nIndex: 942\nTarget: 0\nEnugu Government to demolish illegal structures at International Conference Centre: Enugu State government app... http://t.co/MsKn6D3eKH\n\nIndex: 2843\nTarget: 0\nPeople really still be having curfew even when they're 18 &amp; graduated high school ??\n\nIndex: 1114\nTarget: 1\nSevere Thunderstorm Warning including Russellville AR Clarksville AR Dardanelle AR until 10:15 PM CDT http://t.co/n844h1ASPj\n\nIndex: 3884\nTarget: 1\nA look at state actions a year after Ferguson's upheaval http://t.co/vXUFtVT9AU\n\n\n\n\n\nAdd Features\n\nX_train, y_train = return_xy(train_data)\nX_test, y_test = return_xy(test_data)\n\nfeatures = ['tweet_len', 'word_count', 'word_density', 'links_f', 'neg', 'neu', 'pos', 'compound_f']\n\nGetting polarity scores...\nGetting polarity scores...\n\n\n\n\nTokenize and Vectorize Data\n\n# Runs the custom function to vectorize the data\nX_train_v, X_test_v = return_vectorized_data(X_train, X_test, 'text', features=features, max_features=None, stop_words=stop_words)\n\n\nX_train_v[features]\n\n\n\n\n\n  \n    \n      \n      tweet_len\n      word_count\n      word_density\n      links_f\n      neg\n      neu\n      pos\n      compound_f\n    \n  \n  \n    \n      0\n      140\n      18\n      7.777778\n      1\n      0.207\n      0.536\n      0.257\n      0.3182\n    \n    \n      1\n      98\n      15\n      6.533333\n      0\n      0.205\n      0.795\n      0.000\n      -0.4767\n    \n    \n      2\n      143\n      22\n      6.500000\n      1\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      3\n      118\n      17\n      6.941176\n      1\n      0.138\n      0.862\n      0.000\n      -0.3736\n    \n    \n      4\n      82\n      9\n      9.111111\n      0\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6085\n      96\n      16\n      6.000000\n      0\n      0.000\n      0.502\n      0.498\n      0.9169\n    \n    \n      6086\n      132\n      28\n      4.714286\n      0\n      0.276\n      0.724\n      0.000\n      -0.8126\n    \n    \n      6087\n      121\n      13\n      9.307693\n      1\n      0.155\n      0.845\n      0.000\n      -0.2960\n    \n    \n      6088\n      136\n      20\n      6.800000\n      1\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      6089\n      101\n      17\n      5.941176\n      1\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n  \n\n6090 rows × 8 columns\n\n\n\n\nvectors = [col for col in X_train_v.columns if col not in features]\nX_train_v[vectors]\n\n\n\n\n\n  \n    \n      \n      !\n      #\n      ##book\n      ##youtube\n      #034\n      #039\n      #06\n      #09\n      #1-1st\n      #1008pla\n      ...\n      \n      ¡\n      ¢\n      £\n      ¤\n      ©\n      «\n      ¬\n      ÷\n      ⁄\n    \n  \n  \n    \n      0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.118436\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6085\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      6086\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      6087\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      6088\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      6089\n      0.160096\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n6090 rows × 17781 columns\n\n\n\n\n\nScale Data\n\n# import sklearn standard scaler\nfrom sklearn.preprocessing import StandardScaler\n\n\nX_train_f = X_train_v[features]\nX_test_f = X_test_v[features]\n\n# instantiate the scaler\nscaler = StandardScaler()\n\n# fit transform the training data\nX_train_fs = scaler.fit_transform(X_train_f)\nX_test_fs = scaler.transform(X_test_f)\n\nX_train_fs = pd.DataFrame(X_train_fs, columns=features, index=X_train_f.index)\nX_test_fs = pd.DataFrame(X_test_fs, columns=features, index=X_test_f.index)\n\nX_train_v[features] = X_train_fs[features]\nX_test_v[features] = X_test_fs[features]\n\n\n\nTrain Model\nFirst we convert the dataframes to numpy arrays to work with the keras api. I used the functional api to build the model. It has two hidden layers, a concatenated layer of the original input and previous hidden layers, and one more hidden layer before the output layer. I used the Adam optimizer and the binary crossentropy loss function. I also used the early stopping and checkpoint callbacks to save the best model. I used the fit method to train the model.\n\ntrain_text = X_train_v.drop(features, axis=1)\ntest_text = X_test_v.drop(features, axis=1)\n\ntrain_feats = X_train_v[features]\ntest_feats = X_test_v[features]\n\n\ntrain_text_n = train_text.to_numpy()\ntest_text_n = test_text.to_numpy()\n\ntrain_feats_n = train_feats.to_numpy()\ntest_feats_n = test_feats.to_numpy()\n\ny_train_n = y_train.to_numpy()\ny_test_n = y_test.to_numpy()\n\n\ntf.keras.backend.clear_session()\n\nact1 = tf.keras.layers.LeakyReLU(alpha=0.05)\n\ntext_input_ = tf.keras.layers.Input(shape=(train_text_n.shape[1],), name='text_input')\ntext_layers = tf.keras.layers.Dense(32, activation=act1)(text_input_)\n#text_layers = tf.keras.layers.Dropout(0.2)(text_layers)\n\nfeats_input_ = tf.keras.layers.Input(shape=(train_feats_n.shape[1],), name='feats_input')\nfeats_layers = tf.keras.layers.Dense(32, activation=act1)(feats_input_)\nfeats_layers = tf.keras.layers.Dropout(0.1)(feats_layers)\n\nconcat_l = tf.keras.layers.Concatenate()([text_layers, feats_layers])\nconcat_i = tf.keras.layers.Concatenate()([text_input_, feats_input_])\n\nconcat = tf.keras.layers.Concatenate()([concat_l, concat_i])\nconcat = tf.keras.layers.Dropout(0.1)(concat)\n\noutput_ = tf.keras.layers.Dense(1, activation='sigmoid')(concat)\n\nmodel = tf.keras.Model(inputs=[text_input_, feats_input_], outputs=output_)\n\nmodel.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n feats_input (InputLayer)       [(None, 8)]          0           []                               \n                                                                                                  \n text_input (InputLayer)        [(None, 17781)]      0           []                               \n                                                                                                  \n dense_1 (Dense)                (None, 32)           288         ['feats_input[0][0]']            \n                                                                                                  \n dense (Dense)                  (None, 32)           569024      ['text_input[0][0]']             \n                                                                                                  \n dropout (Dropout)              (None, 32)           0           ['dense_1[0][0]']                \n                                                                                                  \n concatenate (Concatenate)      (None, 64)           0           ['dense[0][0]',                  \n                                                                  'dropout[0][0]']                \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 17789)        0           ['text_input[0][0]',             \n                                                                  'feats_input[0][0]']            \n                                                                                                  \n concatenate_2 (Concatenate)    (None, 17853)        0           ['concatenate[0][0]',            \n                                                                  'concatenate_1[0][0]']          \n                                                                                                  \n dropout_1 (Dropout)            (None, 17853)        0           ['concatenate_2[0][0]']          \n                                                                                                  \n dense_2 (Dense)                (None, 1)            17854       ['dropout_1[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 587,166\nTrainable params: 587,166\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\nopt = tf.keras.optimizers.Adamax(learning_rate=0.0003)\nmetric = tf.keras.metrics.AUC()\n\nmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=metric)\n\n\nimport absl.logging\nabsl.logging.set_verbosity(absl.logging.ERROR)\n\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=4, mode='max'),\n             tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min'),\n             tf.keras.callbacks.ModelCheckpoint(filepath='checkpoint.hd5', monitor='val_auc', save_best_only=True, mode='max')\n             ]\n\nmodel_results = model.fit([train_text, train_feats], y_train_n, validation_split=0.02,\n                          epochs=100, batch_size=64, callbacks=callbacks,\n                          use_multiprocessing=True)\n\nEpoch 1/100\n94/94 [==============================] - ETA: 0s - loss: 0.6846 - auc: 0.6286INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 4s 39ms/step - loss: 0.6846 - auc: 0.6286 - val_loss: 0.6754 - val_auc: 0.7030\nEpoch 2/100\n94/94 [==============================] - ETA: 0s - loss: 0.6690 - auc: 0.7009INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 3s 32ms/step - loss: 0.6690 - auc: 0.7009 - val_loss: 0.6590 - val_auc: 0.7297\nEpoch 3/100\n94/94 [==============================] - ETA: 0s - loss: 0.6553 - auc: 0.7164INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 3s 28ms/step - loss: 0.6553 - auc: 0.7164 - val_loss: 0.6447 - val_auc: 0.7473\nEpoch 4/100\n94/94 [==============================] - ETA: 0s - loss: 0.6438 - auc: 0.7232INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 27ms/step - loss: 0.6438 - auc: 0.7232 - val_loss: 0.6327 - val_auc: 0.7508\nEpoch 5/100\n93/94 [============================>.] - ETA: 0s - loss: 0.6329 - auc: 0.7321INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.6329 - auc: 0.7323 - val_loss: 0.6209 - val_auc: 0.7559\nEpoch 6/100\n93/94 [============================>.] - ETA: 0s - loss: 0.6225 - auc: 0.7389INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 3s 27ms/step - loss: 0.6224 - auc: 0.7394 - val_loss: 0.6105 - val_auc: 0.7620\nEpoch 7/100\n94/94 [==============================] - ETA: 0s - loss: 0.6139 - auc: 0.7446INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.6139 - auc: 0.7446 - val_loss: 0.6012 - val_auc: 0.7672\nEpoch 8/100\n94/94 [==============================] - ETA: 0s - loss: 0.6037 - auc: 0.7547INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.6037 - auc: 0.7547 - val_loss: 0.5927 - val_auc: 0.7718\nEpoch 9/100\n92/94 [============================>.] - ETA: 0s - loss: 0.5983 - auc: 0.7567INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 22ms/step - loss: 0.5973 - auc: 0.7584 - val_loss: 0.5854 - val_auc: 0.7766\nEpoch 10/100\n93/94 [============================>.] - ETA: 0s - loss: 0.5892 - auc: 0.7662INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.5893 - auc: 0.7660 - val_loss: 0.5783 - val_auc: 0.7807\nEpoch 11/100\n93/94 [============================>.] - ETA: 0s - loss: 0.5833 - auc: 0.7704INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.5834 - auc: 0.7701 - val_loss: 0.5718 - val_auc: 0.7842\nEpoch 12/100\n94/94 [==============================] - ETA: 0s - loss: 0.5742 - auc: 0.7810INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.5742 - auc: 0.7810 - val_loss: 0.5656 - val_auc: 0.7907\nEpoch 13/100\n94/94 [==============================] - ETA: 0s - loss: 0.5676 - auc: 0.7875INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.5676 - auc: 0.7875 - val_loss: 0.5598 - val_auc: 0.7955\nEpoch 14/100\n94/94 [==============================] - ETA: 0s - loss: 0.5609 - auc: 0.7945INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.5609 - auc: 0.7945 - val_loss: 0.5544 - val_auc: 0.7990\nEpoch 15/100\n94/94 [==============================] - ETA: 0s - loss: 0.5538 - auc: 0.8022INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 3s 28ms/step - loss: 0.5538 - auc: 0.8022 - val_loss: 0.5489 - val_auc: 0.8049\nEpoch 16/100\n94/94 [==============================] - ETA: 0s - loss: 0.5455 - auc: 0.8120INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.5455 - auc: 0.8120 - val_loss: 0.5435 - val_auc: 0.8090\nEpoch 17/100\n93/94 [============================>.] - ETA: 0s - loss: 0.5376 - auc: 0.8194INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 3s 28ms/step - loss: 0.5378 - auc: 0.8192 - val_loss: 0.5380 - val_auc: 0.8132\nEpoch 18/100\n94/94 [==============================] - ETA: 0s - loss: 0.5304 - auc: 0.8267INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.5304 - auc: 0.8267 - val_loss: 0.5329 - val_auc: 0.8168\nEpoch 19/100\n94/94 [==============================] - ETA: 0s - loss: 0.5220 - auc: 0.8360INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.5220 - auc: 0.8360 - val_loss: 0.5276 - val_auc: 0.8202\nEpoch 20/100\n92/94 [============================>.] - ETA: 0s - loss: 0.5144 - auc: 0.8429INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.5148 - auc: 0.8423 - val_loss: 0.5224 - val_auc: 0.8261\nEpoch 21/100\n94/94 [==============================] - ETA: 0s - loss: 0.5064 - auc: 0.8504INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.5064 - auc: 0.8504 - val_loss: 0.5175 - val_auc: 0.8317\nEpoch 22/100\n94/94 [==============================] - ETA: 0s - loss: 0.4990 - auc: 0.8572INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.4990 - auc: 0.8572 - val_loss: 0.5126 - val_auc: 0.8380\nEpoch 23/100\n94/94 [==============================] - ETA: 0s - loss: 0.4902 - auc: 0.8655INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.4902 - auc: 0.8655 - val_loss: 0.5073 - val_auc: 0.8424\nEpoch 24/100\n94/94 [==============================] - ETA: 0s - loss: 0.4816 - auc: 0.8733INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.4816 - auc: 0.8733 - val_loss: 0.5028 - val_auc: 0.8452\nEpoch 25/100\n94/94 [==============================] - ETA: 0s - loss: 0.4743 - auc: 0.8796INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 3s 29ms/step - loss: 0.4743 - auc: 0.8796 - val_loss: 0.4984 - val_auc: 0.8482\nEpoch 26/100\n93/94 [============================>.] - ETA: 0s - loss: 0.4670 - auc: 0.8860INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.4667 - auc: 0.8861 - val_loss: 0.4940 - val_auc: 0.8493\nEpoch 27/100\n94/94 [==============================] - ETA: 0s - loss: 0.4596 - auc: 0.8910INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 22ms/step - loss: 0.4596 - auc: 0.8910 - val_loss: 0.4895 - val_auc: 0.8521\nEpoch 28/100\n93/94 [============================>.] - ETA: 0s - loss: 0.4505 - auc: 0.8981INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.4504 - auc: 0.8982 - val_loss: 0.4851 - val_auc: 0.8556\nEpoch 29/100\n94/94 [==============================] - ETA: 0s - loss: 0.4438 - auc: 0.9017INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.4438 - auc: 0.9017 - val_loss: 0.4807 - val_auc: 0.8581\nEpoch 30/100\n94/94 [==============================] - ETA: 0s - loss: 0.4357 - auc: 0.9072INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 26ms/step - loss: 0.4357 - auc: 0.9072 - val_loss: 0.4766 - val_auc: 0.8592\nEpoch 31/100\n94/94 [==============================] - ETA: 0s - loss: 0.4285 - auc: 0.9120INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 3s 27ms/step - loss: 0.4285 - auc: 0.9120 - val_loss: 0.4726 - val_auc: 0.8610\nEpoch 32/100\n89/94 [===========================>..] - ETA: 0s - loss: 0.4195 - auc: 0.9183INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.4199 - auc: 0.9176 - val_loss: 0.4686 - val_auc: 0.8626\nEpoch 33/100\n94/94 [==============================] - ETA: 0s - loss: 0.4126 - auc: 0.9218INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.4126 - auc: 0.9218 - val_loss: 0.4648 - val_auc: 0.8642\nEpoch 34/100\n94/94 [==============================] - 1s 12ms/step - loss: 0.4058 - auc: 0.9249 - val_loss: 0.4612 - val_auc: 0.8629\nEpoch 35/100\n93/94 [============================>.] - ETA: 0s - loss: 0.3988 - auc: 0.9282INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.3989 - auc: 0.9282 - val_loss: 0.4580 - val_auc: 0.8645\nEpoch 36/100\n93/94 [============================>.] - ETA: 0s - loss: 0.3915 - auc: 0.9323INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 27ms/step - loss: 0.3916 - auc: 0.9322 - val_loss: 0.4547 - val_auc: 0.8656\nEpoch 37/100\n94/94 [==============================] - ETA: 0s - loss: 0.3845 - auc: 0.9364INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.3845 - auc: 0.9364 - val_loss: 0.4515 - val_auc: 0.8675\nEpoch 38/100\n94/94 [==============================] - ETA: 0s - loss: 0.3781 - auc: 0.9384INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.3781 - auc: 0.9384 - val_loss: 0.4484 - val_auc: 0.8682\nEpoch 39/100\n91/94 [============================>.] - ETA: 0s - loss: 0.3742 - auc: 0.9393INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.3733 - auc: 0.9399 - val_loss: 0.4457 - val_auc: 0.8697\nEpoch 40/100\n93/94 [============================>.] - ETA: 0s - loss: 0.3657 - auc: 0.9435INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.3657 - auc: 0.9435 - val_loss: 0.4426 - val_auc: 0.8701\nEpoch 41/100\n94/94 [==============================] - ETA: 0s - loss: 0.3610 - auc: 0.9445INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.3610 - auc: 0.9445 - val_loss: 0.4398 - val_auc: 0.8730\nEpoch 42/100\n92/94 [============================>.] - ETA: 0s - loss: 0.3548 - auc: 0.9471INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 22ms/step - loss: 0.3538 - auc: 0.9478 - val_loss: 0.4370 - val_auc: 0.8737\nEpoch 43/100\n94/94 [==============================] - ETA: 0s - loss: 0.3470 - auc: 0.9504INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.3470 - auc: 0.9504 - val_loss: 0.4340 - val_auc: 0.8744\nEpoch 44/100\n93/94 [============================>.] - ETA: 0s - loss: 0.3415 - auc: 0.9525INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.3415 - auc: 0.9526 - val_loss: 0.4315 - val_auc: 0.8745\nEpoch 45/100\n92/94 [============================>.] - ETA: 0s - loss: 0.3359 - auc: 0.9542INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.3364 - auc: 0.9540 - val_loss: 0.4291 - val_auc: 0.8767\nEpoch 46/100\n94/94 [==============================] - ETA: 0s - loss: 0.3300 - auc: 0.9557INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.3300 - auc: 0.9557 - val_loss: 0.4269 - val_auc: 0.8797\nEpoch 47/100\n92/94 [============================>.] - ETA: 0s - loss: 0.3246 - auc: 0.9574INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.3245 - auc: 0.9575 - val_loss: 0.4248 - val_auc: 0.8797\nEpoch 48/100\n94/94 [==============================] - ETA: 0s - loss: 0.3182 - auc: 0.9601INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 22ms/step - loss: 0.3182 - auc: 0.9601 - val_loss: 0.4224 - val_auc: 0.8802\nEpoch 49/100\n94/94 [==============================] - ETA: 0s - loss: 0.3139 - auc: 0.9606INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.3139 - auc: 0.9606 - val_loss: 0.4205 - val_auc: 0.8819\nEpoch 50/100\n94/94 [==============================] - 1s 12ms/step - loss: 0.3082 - auc: 0.9624 - val_loss: 0.4182 - val_auc: 0.8815\nEpoch 51/100\n94/94 [==============================] - ETA: 0s - loss: 0.3038 - auc: 0.9631INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.3038 - auc: 0.9631 - val_loss: 0.4163 - val_auc: 0.8820\nEpoch 52/100\n92/94 [============================>.] - ETA: 0s - loss: 0.2997 - auc: 0.9642INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.2996 - auc: 0.9644 - val_loss: 0.4142 - val_auc: 0.8830\nEpoch 53/100\n94/94 [==============================] - ETA: 0s - loss: 0.2939 - auc: 0.9653INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.2939 - auc: 0.9653 - val_loss: 0.4122 - val_auc: 0.8841\nEpoch 54/100\n94/94 [==============================] - ETA: 0s - loss: 0.2898 - auc: 0.9667INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.2898 - auc: 0.9667 - val_loss: 0.4108 - val_auc: 0.8852\nEpoch 55/100\n94/94 [==============================] - ETA: 0s - loss: 0.2841 - auc: 0.9685INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.2841 - auc: 0.9685 - val_loss: 0.4093 - val_auc: 0.8861\nEpoch 56/100\n93/94 [============================>.] - ETA: 0s - loss: 0.2801 - auc: 0.9694INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.2802 - auc: 0.9694 - val_loss: 0.4078 - val_auc: 0.8868\nEpoch 57/100\n92/94 [============================>.] - ETA: 0s - loss: 0.2773 - auc: 0.9699INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.2768 - auc: 0.9700 - val_loss: 0.4064 - val_auc: 0.8869\nEpoch 58/100\n93/94 [============================>.] - ETA: 0s - loss: 0.2719 - auc: 0.9712INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.2720 - auc: 0.9711 - val_loss: 0.4052 - val_auc: 0.8872\nEpoch 59/100\n92/94 [============================>.] - ETA: 0s - loss: 0.2678 - auc: 0.9726INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.2676 - auc: 0.9724 - val_loss: 0.4036 - val_auc: 0.8882\nEpoch 60/100\n94/94 [==============================] - 1s 13ms/step - loss: 0.2631 - auc: 0.9734 - val_loss: 0.4025 - val_auc: 0.8879\nEpoch 61/100\n94/94 [==============================] - 1s 13ms/step - loss: 0.2604 - auc: 0.9736 - val_loss: 0.4010 - val_auc: 0.8882\nEpoch 62/100\n94/94 [==============================] - 1s 12ms/step - loss: 0.2566 - auc: 0.9742 - val_loss: 0.3999 - val_auc: 0.8882\nEpoch 63/100\n94/94 [==============================] - ETA: 0s - loss: 0.2510 - auc: 0.9756INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.2510 - auc: 0.9756 - val_loss: 0.3986 - val_auc: 0.8884\nEpoch 64/100\n94/94 [==============================] - ETA: 0s - loss: 0.2468 - auc: 0.9766INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.2468 - auc: 0.9766 - val_loss: 0.3976 - val_auc: 0.8884\nEpoch 65/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.2421 - auc: 0.9775 - val_loss: 0.3966 - val_auc: 0.8884\nEpoch 66/100\n94/94 [==============================] - ETA: 0s - loss: 0.2400 - auc: 0.9778INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 21ms/step - loss: 0.2400 - auc: 0.9778 - val_loss: 0.3955 - val_auc: 0.8895\nEpoch 67/100\n94/94 [==============================] - 1s 16ms/step - loss: 0.2371 - auc: 0.9783 - val_loss: 0.3947 - val_auc: 0.8893\nEpoch 68/100\n94/94 [==============================] - ETA: 0s - loss: 0.2338 - auc: 0.9789INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.2338 - auc: 0.9789 - val_loss: 0.3934 - val_auc: 0.8898\nEpoch 69/100\n94/94 [==============================] - 1s 12ms/step - loss: 0.2287 - auc: 0.9803 - val_loss: 0.3927 - val_auc: 0.8893\nEpoch 70/100\n94/94 [==============================] - ETA: 0s - loss: 0.2254 - auc: 0.9807INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 25ms/step - loss: 0.2254 - auc: 0.9807 - val_loss: 0.3917 - val_auc: 0.8905\nEpoch 71/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.2216 - auc: 0.9813 - val_loss: 0.3911 - val_auc: 0.8903\nEpoch 72/100\n93/94 [============================>.] - ETA: 0s - loss: 0.2185 - auc: 0.9823INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 22ms/step - loss: 0.2186 - auc: 0.9823 - val_loss: 0.3902 - val_auc: 0.8908\nEpoch 73/100\n93/94 [============================>.] - ETA: 0s - loss: 0.2152 - auc: 0.9827INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 24ms/step - loss: 0.2152 - auc: 0.9827 - val_loss: 0.3893 - val_auc: 0.8912\nEpoch 74/100\n94/94 [==============================] - ETA: 0s - loss: 0.2119 - auc: 0.9831INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 26ms/step - loss: 0.2119 - auc: 0.9831 - val_loss: 0.3887 - val_auc: 0.8925\nEpoch 75/100\n94/94 [==============================] - 1s 13ms/step - loss: 0.2099 - auc: 0.9832 - val_loss: 0.3882 - val_auc: 0.8920\nEpoch 76/100\n93/94 [============================>.] - ETA: 0s - loss: 0.2069 - auc: 0.9837INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n94/94 [==============================] - 2s 23ms/step - loss: 0.2067 - auc: 0.9838 - val_loss: 0.3870 - val_auc: 0.8938\nEpoch 77/100\n94/94 [==============================] - 1s 12ms/step - loss: 0.2039 - auc: 0.9846 - val_loss: 0.3867 - val_auc: 0.8928\nEpoch 78/100\n94/94 [==============================] - 1s 16ms/step - loss: 0.1999 - auc: 0.9854 - val_loss: 0.3859 - val_auc: 0.8931\nEpoch 79/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.1976 - auc: 0.9856 - val_loss: 0.3856 - val_auc: 0.8936\nEpoch 80/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.1952 - auc: 0.9858 - val_loss: 0.3855 - val_auc: 0.8932\n\n\n\nplot_history(model_results)\n\n\n\n\nAbove is a plot of the training and validation accuracy and loss. The model seems to be overfitting more as the epochs increase, and we can see the best epoch for loss is around 79, where the best epoch for accuracy is around 76. This indicates that the model may be overfitting and may not perform well when deployed in the real world. However, the model may also be improved by changing the optimizer, learning rate, adding more layers or the amount of training data avaliable.\n\n\nEvaluate Model\n\nmodel.load_weights('checkpoint.hd5')\n\nmodel.evaluate([test_text, test_feats], y_test_n)\n\n48/48 [==============================] - 2s 35ms/step - loss: 0.4505 - auc: 0.8625\n\n\n[0.45050153136253357, 0.8624657392501831]\n\n\n\nplot_roc([train_text, train_feats], [test_text, test_feats], y_train_n, y_test_n, model)\n\n191/191 [==============================] - 6s 33ms/step\n48/48 [==============================] - 2s 36ms/step\n\n\n\n\n\nAbove we see the results for our test data set. The model has an AUC of .86, which is not bad. However, from the ROC AUC plot above we can see that the model is still overfitting to the training data and is not generalizing to the test data as well as it could. This could be due to many factors, such as the amount or accuracy of training data, the amount of features, the model architecture, the learning rate, and the amount of epochs. However, the model is still able to predict wheather or not a tweet is about a real disaster with a decent accuracy."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Below is a list of links to different projects of mine that I have selected to hopefully provide a demonstration of the breadth of skills and knowledge I have gained throughout my acedemic and professional experience."
  },
  {
    "objectID": "portfolio.html#projects",
    "href": "portfolio.html#projects",
    "title": "Portfolio",
    "section": "Projects",
    "text": "Projects\nNatural Language Processing and Neural Network Classification\nAnother Placeholder Project"
  }
]