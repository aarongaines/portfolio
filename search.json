[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Aaron Gaines, and I am 29 years old. I have been working as a research assistant at California State University, Northridge (CSUN) since October 2019, where I also received a Bachelors in Geography - Geographic Information Science in June 2021. I have also recently received a graduate certificate in Data Science from the UCLA Extension program.\n\nExperience\nMost of my time as a research assistant has been spent working on updating the National Hydrography Dataset (NHD), a USGS database, for the California Department of Water Resources. This involved creating, editing, and managing digitized hydrographic features while maintaining a minimum level of quality required by the USGS to ensure data interoperability between different state agency’s update efforts. Overtime my responsibilities within the team grew from simple first pass editing to managing a job from start to finish, including performing quality control checks on other editor’s work to ensure our updates can be merged into the main database.\nMore recently I have been working on a pilot project investigating the possible effects of sea level rise on groundwater levels and contamination, as well as disadvantaged communities (DACs). This has involved doing literature reviews on similar projects, aggregating, and presenting that information to a larger team. With feedback I then would apply the methodologies found in different papers to our data to compare and discover the different advantages and drawbacks. An example of this was to look at how different state agencies would designate DACs. From this the group decided on a methodology that seemed to be the most complete, which happened to be the CalEnviroScreen (CES). I then used their white paper to emulate some of their methodologies in determining environmental and social disadvantages with python and ArcGIS Pro. For example, spatial analysis of monitoring wells and their sample data to determine a blockgroup’s exposure to specific groundwater contamination. When we found their determination of data quality not to our liking, I used the American Housing Survey variance replicate data to determine the uncertainty in our social vulnerability index, a method described by (et al.). This project has grown my skills in literature review, effectively presenting technical papers and methodologies and emulation of said methodologies with Python.\n\n\nSkills\nI am very proficient in the use of the ESRI stack of products, particularly ArcGIS Pro and ArcGIS Online as well as automating tasks using the ArcGIS model builder and ArcPy. While I do not have a lot of experience with QGIS, that is something I would like to change as I have a fondness for opensource technology for the simple fact that it allows others to replicate and expand on the work of others more easily. I also have a decent amount of experience in Python and many of its data science libraries including Pandas, Matplotlib, Scikit-Learn and the Tensorflow Keras API. I have no problem writing custom classes or modules/packages where necessary. I also have some experience with PostgreSQL and PostGIS queries.\n\n\nWhy GIS and Data Science?\nI enjoy looking at problems from a big picture perspective, and I really enjoy how GIS and data science let me explore and solve problems concerning such a wide range of industries and topics. And while I am constantly learning new things directly related to my work, like tricking machine learning models, I also enjoy learning about a vast range of other topics related to science. Whether its material science or astrophysics, I am always intrigued by the process solving puzzles and discovering new things."
  },
  {
    "objectID": "nlp_ann.html",
    "href": "nlp_ann.html",
    "title": "Aaron Gaines",
    "section": "",
    "text": "Natural Language Processing (NLP) and Neural Networks with NLTK and the Tensorflow Keras API\n\nIntroduction\nIn this notebook, I will be building a model to predict whether or not a tweet is about a real disaster, using the data given for the assignment. The data has been provided in separate training and test sets, so only the training set will be used in the model training/validation while the test set will be saved for the end to evalute the model.\nFirst the data will be loaded and explored, then the data will be cleaned and preprocessed, and finally the model will be built and trained. I decided to add some features to the data set (e.g. number of words in a tweet) to see if they would improve the model’s performance. I also used the prebuild sentiment analysis model from the NLTK library to see if it would improve the model’s performance. Finally I used the sklearn library to vectorize the text data with the TF-IDF vectorizer and the nltk tweet tokenizer to tokenize the text data. After all of this, I used the tensorflow.keras library to build and train the model.\n\n\nImport Libraries\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import TweetTokenizer\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_curve\nimport random\n\n\nrandom.seed(42)\n\n\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nTrue\n\n\n\nstop_words = set(stopwords.words('english'))\n\n\n\nCustom Functions\nlist of custom functions:\n\ndtype_convert: converts the data type of a column in a dataframe\nlink_count: if a link exists in a tweet\nget_polarity: returns the polarity of a tweet using the NLTK sentiment analysis model\nreturn_xy: returns the x and y data from a dataframe\nreturn_vectorized_data: returns the vectorized text and extra features from a dataframe\nplot_history: plots the training and validation accuracy and loss from a history object\nplot_roc: plots the ROC curve using the best saved model and the test data\n\n\ndef dtype_convert(data):\n\n    int_list = ['int8', 'int16', 'int32', 'int64']\n    float_list = ['float16', 'float32', 'float64']\n\n    for col in data.select_dtypes(include=np.number).columns:\n\n        data_type = data[col].dtype\n        \n        if data_type in int_list:\n\n            n_range = [abs(data[col].max()), abs(data[col].min())]\n            n_max = max(n_range)\n\n            if n_max <= 127:\n                data[col] = data[col].astype(np.int8)\n\n            elif n_max <= 32767:\n                data[col] = data[col].astype(np.int16)\n\n            elif n_max <= 2147483647:\n                data[col] = data[col].astype(np.int32)\n\n            else:\n                data[col] = data[col].astype(np.int64)\n\n        elif data_type in float_list:\n                \n                n_range = [abs(data[col].max()), abs(data[col].min())]\n                n_max = max(n_range)\n    \n                if n_max <= 3.4e+38:\n                    data[col] = data[col].astype(np.float32)\n    \n                else:\n                    data[col] = data[col].astype(np.float64)\n    \n    return data\n        \n\n\ndef link_count(text):\n\n    links = text.count('http')\n\n    if links > 0:\n\n        return 1\n    \n    else:\n\n        return 0\n\n\ndef get_polarity(row):\n\n    sia = SentimentIntensityAnalyzer()\n\n    text = row['text']\n    polarity = sia.polarity_scores(text)\n\n    neg = polarity['neg']\n    neu = polarity['neu']\n    pos = polarity['pos']\n    compound = polarity['compound']\n\n    return neg, neu, pos, compound\n\n\ndef return_xy(data):\n\n    # Add new features including polarity scores\n    # Binary feature for whether a tweet contains a link\n    data['links'] = data['text'].apply(link_count).astype(np.int8)\n    \n    data['tweet_len'] = data['text'].str.len()\n    data['word_count'] = data['text'].str.split().str.len()\n    data['word_density'] = data['tweet_len'] / (data['word_count'])\n\n    # Get polarity scores\n    print('Getting polarity scores...')\n    data['neg'], data['neu'], data['pos'], data['compound'] = zip(*data.apply(get_polarity, axis=1))\n\n    # Convert dtypes to save memory\n    #data['target'] = data['target'].astype('bool')\n    data = dtype_convert(data)\n    \n    # Columns for X and y\n    X = data.drop('target', axis=1)\n    y = data['target'].astype('int8')\n\n    return X, y\n\n\ndef return_vectorized_data(train, test, text, features, max_features=None, stop_words=None):\n\n    tk = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n\n    vectorizer = TfidfVectorizer(tokenizer=tk.tokenize, strip_accents='unicode', max_features=max_features, stop_words=stop_words)\n\n    v_train = vectorizer.fit_transform(train[text])\n    v_test = vectorizer.transform(test[text])\n\n    v_train = pd.DataFrame(v_train.toarray(), columns=vectorizer.get_feature_names_out(), index=train.index)\n    v_test = pd.DataFrame(v_test.toarray(), columns=vectorizer.get_feature_names_out(), index=test.index)\n\n    v_train = v_train[v_train.columns].astype(np.float32)\n    v_test = v_test[v_test.columns].astype(np.float32)\n\n    v_train = v_train.join(train[features], how='left', lsuffix='_v', rsuffix='_f')\n    v_test = v_test.join(test[features], how='left', lsuffix='_v', rsuffix='_f')\n\n    return v_train, v_test\n\n\ndef plot_history(history):\n\n    fig, ax = plt.subplots(2, 1, figsize=(11, 11))\n\n    auc_best = np.max(history.history['val_auc'])\n    auc_best_epoch = np.argmax(history.history['val_auc'])\n    auc_max = history.history['auc'][auc_best_epoch]\n\n\n    ax[0].plot(history.history['auc'], label='Train')\n    ax[0].plot(history.history['val_auc'], label='Validation')\n    ax[0].vlines(auc_best_epoch, 0, np.max(history.history['val_auc']), color='purple', linestyles='dashed', label='Best Epoch')\n\n    ax[0].annotate(f'{auc_best - np.max(history.history[\"auc\"]):.4f}',\n                    xy=(auc_best_epoch, auc_max),\n                    xytext=(auc_best_epoch, auc_best-0.05),\n                    arrowprops=dict(facecolor='black', shrink=0.05, width=0.7, headwidth=5))\n\n    ax[0].set_ylabel('AUC')\n    #ax[0].set_title('AUC vs. Epoch')\n    ax[0].legend(loc='lower left')\n\n    loss_best = np.min(history.history['val_loss'])\n    loss_best_epoch = np.argmin(history.history['val_loss'])\n    loss_max = history.history['loss'][loss_best_epoch]\n\n    ax[1].plot(history.history['loss'], label='Train')\n    ax[1].plot(history.history['val_loss'], label='Validation')\n    ax[1].vlines(loss_best_epoch, 0, loss_best, color='purple', linestyles='dashed', label='Best Epoch')\n\n    ax[1].annotate(f'{loss_best - np.min(history.history[\"loss\"]):.4f}',\n                    xy=(loss_best_epoch, loss_max),\n                    xytext=(loss_best_epoch, loss_best+.01),\n                    arrowprops=dict(facecolor='black', shrink=0, width=0.7, headwidth=5))\n\n    ax[1].set_xlabel('Epoch')\n    ax[1].set_ylabel('Loss')\n    #ax[1].set_title('Loss vs. Epoch')\n\n    ax[1].legend(loc='lower left')\n\n    plt.show()\n\n\ndef plot_roc(X_train, X_test, y_train, y_test, model):\n    \n    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, model.predict(X_train))\n    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, model.predict(X_test))\n\n    # Set figure size\n    plt.figure(figsize=(10, 10))\n\n    plt.plot(fpr_train, tpr_train, label='Train')\n    plt.plot(fpr_test, tpr_test, label='Test')\n\n    plt.legend(['Train', 'Test'], loc='upper left')\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC AUC Curve')\n    plt.show()\n\n\n\nImport Data\n\ntrain_data = pd.read_csv('bonus_train_data-2.csv')\ntest_data = pd.read_csv('bonus_test_data-2.csv')\n\n\n\nAdd Features\n\nX_train, y_train = return_xy(train_data)\nX_test, y_test = return_xy(test_data)\n\nfeatures = ['tweet_len', 'word_count', 'word_density', 'links', 'neg', 'neu', 'pos', 'compound']\n\nGetting polarity scores...\nGetting polarity scores...\n\n\n\n\nTokenize and Vectorize Data\n\n# Runs the custom function to vectorize the data\nX_train_v, X_test_v = return_vectorized_data(X_train, X_test, 'text', features=features, max_features=None, stop_words=stop_words)\n\n\n\nTrain Model\nFirst we convert the dataframes to numpy arrays to work with the keras api. I used the functional api to build the model. It has two hidden layers, a concatenated layer of the original input and previous hidden layers, and one more hidden layer before the output layer. I used the Adam optimizer and the binary crossentropy loss function. I also used the early stopping and checkpoint callbacks to save the best model. I used the fit method to train the model.\n\n# Convert to numpy arrays\nX_train_v_n = X_train_v.to_numpy()\nX_test_v_n = X_test_v.to_numpy()\n\ny_train_n = y_train.to_numpy()\ny_test_n = y_test.to_numpy()\n\n\ntf.keras.backend.clear_session()\n\nact1 = tf.keras.layers.LeakyReLU(alpha=0.05)\n\n#act1 = tf.keras.activations.\n\ninput_ = tf.keras.layers.Input(shape=(X_train_v_n.shape[1],))\n\nx = tf.keras.layers.Dense(32, activation=act1)(input_)\nx = tf.keras.layers.Dropout(0.2)(x)\n\nx = tf.keras.layers.Dense(32, activation=act1)(x)\nx = tf.keras.layers.Dropout(0.2)(x)\n\nx = tf.keras.layers.Concatenate()([x, input_])\nx = tf.keras.layers.Dense(32, activation=act1)(x)\nx = tf.keras.layers.Dropout(0.2)(x)\n\noutput_ = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.models.Model(input_, output_)\n\nmodel.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 17789)]      0           []                               \n                                                                                                  \n dense (Dense)                  (None, 32)           569280      ['input_1[0][0]']                \n                                                                                                  \n dropout (Dropout)              (None, 32)           0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 32)           1056        ['dropout[0][0]']                \n                                                                                                  \n dropout_1 (Dropout)            (None, 32)           0           ['dense_1[0][0]']                \n                                                                                                  \n concatenate (Concatenate)      (None, 17821)        0           ['dropout_1[0][0]',              \n                                                                  'input_1[0][0]']                \n                                                                                                  \n dense_2 (Dense)                (None, 32)           570304      ['concatenate[0][0]']            \n                                                                                                  \n dropout_2 (Dropout)            (None, 32)           0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 1)            33          ['dropout_2[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 1,140,673\nTrainable params: 1,140,673\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0005)\nmetric = tf.keras.metrics.AUC()\n\nmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=metric)\n\n\nimport absl.logging\nabsl.logging.set_verbosity(absl.logging.ERROR)\n\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=4, mode='max'),\n             tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min'),\n             tf.keras.callbacks.ModelCheckpoint(filepath='checkpoint.hd5', monitor='val_auc', save_best_only=True, mode='max')\n             ]\n\nmodel_results = model.fit(X_train_v_n, y_train_n, validation_split=0.02,\n                          epochs=100, batch_size=96, callbacks=callbacks,\n                          use_multiprocessing=True)\n\nEpoch 1/100\n63/63 [==============================] - ETA: 0s - loss: 0.7552 - auc: 0.5064INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n63/63 [==============================] - 4s 50ms/step - loss: 0.7552 - auc: 0.5064 - val_loss: 0.6654 - val_auc: 0.7341\nEpoch 2/100\n63/63 [==============================] - ETA: 0s - loss: 0.6623 - auc: 0.6269INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n63/63 [==============================] - 3s 45ms/step - loss: 0.6623 - auc: 0.6269 - val_loss: 0.6349 - val_auc: 0.8266\nEpoch 3/100\n63/63 [==============================] - 2s 32ms/step - loss: 0.6148 - auc: 0.7506 - val_loss: 0.6101 - val_auc: 0.7611\nEpoch 4/100\n63/63 [==============================] - 1s 14ms/step - loss: 0.5665 - auc: 0.8106 - val_loss: 0.5556 - val_auc: 0.8240\nEpoch 5/100\n63/63 [==============================] - 1s 15ms/step - loss: 0.5228 - auc: 0.8275 - val_loss: 0.5233 - val_auc: 0.8255\nEpoch 6/100\n63/63 [==============================] - ETA: 0s - loss: 0.4515 - auc: 0.8760INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n63/63 [==============================] - 2s 39ms/step - loss: 0.4515 - auc: 0.8760 - val_loss: 0.4697 - val_auc: 0.8514\nEpoch 7/100\n63/63 [==============================] - ETA: 0s - loss: 0.3669 - auc: 0.9210INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n63/63 [==============================] - 2s 35ms/step - loss: 0.3669 - auc: 0.9210 - val_loss: 0.4307 - val_auc: 0.8711\nEpoch 8/100\n63/63 [==============================] - ETA: 0s - loss: 0.2691 - auc: 0.9578INFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\nINFO:tensorflow:Assets written to: checkpoint.hd5\\assets\n\n\n63/63 [==============================] - 2s 35ms/step - loss: 0.2691 - auc: 0.9578 - val_loss: 0.4717 - val_auc: 0.8841\nEpoch 9/100\n63/63 [==============================] - 2s 36ms/step - loss: 0.2061 - auc: 0.9737 - val_loss: 0.5119 - val_auc: 0.8630\nEpoch 10/100\n63/63 [==============================] - 1s 19ms/step - loss: 0.1916 - auc: 0.9755 - val_loss: 0.5403 - val_auc: 0.8592\n\n\n\nplot_history(model_results)\n\n\n\n\nAbove is a plot of the training and validation accuracy and loss. The model seems to be overfitting more as the epochs increase, and we can see the best epoch for loss is around 6, where the best epoch for accuracy is around 7. This indicates that the model may be overfitting and we should stop training the model at around 6 epochs or 7 epochs. However, the model may also be improved by changing the optimizer, learning rate, adding more layers or the amount of training data avaliable.\n\n\nEvaluate Model\n\nmodel.load_weights('checkpoint.hd5')\n\nmodel.evaluate(X_test_v_n, y_test_n)\n\n48/48 [==============================] - 3s 61ms/step - loss: 0.5063 - auc: 0.8619\n\n\n[0.5062516331672668, 0.8618751764297485]\n\n\n\nplot_roc(X_train_v_n, X_test_v_n, y_train_n, y_test_n, model)\n\n191/191 [==============================] - 11s 57ms/step\n48/48 [==============================] - 3s 63ms/step\n\n\n\n\n\nAbove we see the results for our test data set. The model has an AUC of .862, which is not bad. However, from the ROC AUC plot above we can see that the model is still overfitting to the training data and is not generalizing to the test data as well as it could. This could be due to many factors, such as the amount or accuracy of training data, the amount of features, the model architecture, the learning rate, and the amount of epochs. However, the model is still able to predict wheather or not a tweet is about a real disaster with a decent accuracy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Hi there, my name is Aaron Gaines. This website serves as somewhat of an online portfolio for me. Hopefully you will get a better idea of who I am and the skills I can bring to your projects. The portfolio page contains links to different projects I have completed with the goal of demonstrating as many of my skills in GIS and data science as possible. Please check out my About Me page for more information on who I am and check my Contact links for ways to get in touch with me.\nAdding this line to test the automatic update and publishing."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Below is a list of links to different projects of mine that I have selected to hopefully provide a demonstration of the breadth of skills and knowledge I have gained throughout my acedemic and professional experience."
  },
  {
    "objectID": "portfolio.html#projects",
    "href": "portfolio.html#projects",
    "title": "Portfolio",
    "section": "Projects",
    "text": "Projects\nNatural Language Processing and Neural Network Classification\nAnother Placeholder Project"
  }
]